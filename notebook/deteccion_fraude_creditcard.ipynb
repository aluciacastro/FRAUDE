{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detección de Fraude en Transacciones con Tarjetas de Crédito\n",
    "## Metodología CRISP-DM\n",
    "\n",
    "**Dataset:** Credit Card Fraud Detection (Kaggle)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Comprensión del Negocio\n",
    "\n",
    "### Problema\n",
    "El fraude con tarjetas de crédito representa pérdidas millonarias para instituciones financieras y afecta la confianza de los clientes. Es crucial identificar transacciones fraudulentas de manera automática y en tiempo real.\n",
    "\n",
    "### Objetivo\n",
    "Desarrollar un modelo de machine learning que permita identificar transacciones fraudulentas con alta precisión, minimizando falsos positivos (transacciones legítimas marcadas como fraude) y falsos negativos (fraudes no detectados).\n",
    "\n",
    "### Contexto del Dataset\n",
    "- Contiene transacciones realizadas con tarjetas de crédito en septiembre de 2013 por usuarios europeos\n",
    "- Las transacciones ocurrieron en un período de 2 días\n",
    "- Dataset altamente desbalanceado: solo 0.172% de las transacciones son fraudulentas\n",
    "- Por razones de confidencialidad, las características originales han sido transformadas usando PCA\n",
    "\n",
    "### Métricas de Éxito\n",
    "- **Recall (Sensibilidad):** Capacidad de detectar fraudes reales\n",
    "- **Precision:** Evitar falsos positivos que afecten la experiencia del cliente\n",
    "- **F1-Score:** Balance entre precision y recall\n",
    "- **AUC-ROC:** Capacidad general de discriminación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de librerías necesarias\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    roc_auc_score, roc_curve, precision_recall_curve,\n",
    "    f1_score, accuracy_score, precision_score, recall_score\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Configuración de visualización\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"✓ Librerías importadas exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Comprensión de los Datos\n",
    "\n",
    "En esta fase exploraremos y entenderemos la estructura, calidad y características del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del dataset\n",
    "# NOTA: Descarga el dataset de Kaggle: https://www.kaggle.com/mlg-ulb/creditcardfraud\n",
    "# y colócalo en la ruta apropiada\n",
    "\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "\n",
    "print(\"Dataset cargado exitosamente\")\n",
    "print(f\"Dimensiones: {df.shape[0]} filas x {df.shape[1]} columnas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeras filas del dataset\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información general del dataset\n",
    "print(\"=\" * 60)\n",
    "print(\"INFORMACIÓN GENERAL DEL DATASET\")\n",
    "print(\"=\" * 60)\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ESTADÍSTICAS DESCRIPTIVAS\")\n",
    "print(\"=\" * 60)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de valores nulos\n",
    "print(\"Valores nulos por columna:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nTotal de valores nulos: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis del desbalance de clases\n",
    "print(\"=\" * 60)\n",
    "print(\"ANÁLISIS DE LA VARIABLE OBJETIVO (Class)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class_distribution = df['Class'].value_counts()\n",
    "print(\"\\nConteo de clases:\")\n",
    "print(class_distribution)\n",
    "\n",
    "print(\"\\nProporción de clases:\")\n",
    "print(df['Class'].value_counts(normalize=True) * 100)\n",
    "\n",
    "fraud_percentage = (df['Class'].sum() / len(df)) * 100\n",
    "print(f\"\\nPorcentaje de fraudes: {fraud_percentage:.3f}%\")\n",
    "print(f\"Ratio de desbalance: 1:{int(class_distribution[0]/class_distribution[1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización del desbalance de clases\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Gráfico de barras\n",
    "class_counts = df['Class'].value_counts()\n",
    "axes[0].bar(['Normal', 'Fraude'], class_counts.values, color=['#2ecc71', '#e74c3c'])\n",
    "axes[0].set_ylabel('Cantidad de Transacciones', fontsize=12)\n",
    "axes[0].set_title('Distribución de Clases', fontsize=14, fontweight='bold')\n",
    "axes[0].set_yscale('log')\n",
    "for i, v in enumerate(class_counts.values):\n",
    "    axes[0].text(i, v, f'{v:,}', ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "# Gráfico de pastel\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "explode = (0, 0.1)\n",
    "axes[1].pie(class_counts.values, labels=['Normal', 'Fraude'], autopct='%1.3f%%',\n",
    "            colors=colors, explode=explode, shadow=True, startangle=90)\n",
    "axes[1].set_title('Proporción de Clases', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de las variables Time y Amount\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Distribución de Time\n",
    "axes[0, 0].hist(df['Time'], bins=50, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Tiempo (segundos)', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Frecuencia', fontsize=11)\n",
    "axes[0, 0].set_title('Distribución de Time', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Distribución de Amount\n",
    "axes[0, 1].hist(df['Amount'], bins=50, color='lightcoral', edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Monto ($)', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Frecuencia', fontsize=11)\n",
    "axes[0, 1].set_title('Distribución de Amount', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].set_xlim([0, 1000])\n",
    "\n",
    "# Time por clase\n",
    "df[df['Class'] == 0]['Time'].hist(bins=50, alpha=0.6, label='Normal', ax=axes[1, 0], color='green')\n",
    "df[df['Class'] == 1]['Time'].hist(bins=50, alpha=0.6, label='Fraude', ax=axes[1, 0], color='red')\n",
    "axes[1, 0].set_xlabel('Tiempo (segundos)', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Frecuencia', fontsize=11)\n",
    "axes[1, 0].set_title('Distribución de Time por Clase', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Amount por clase\n",
    "df[df['Class'] == 0]['Amount'].hist(bins=50, alpha=0.6, label='Normal', ax=axes[1, 1], color='green')\n",
    "df[df['Class'] == 1]['Amount'].hist(bins=50, alpha=0.6, label='Fraude', ax=axes[1, 1], color='red')\n",
    "axes[1, 1].set_xlabel('Monto ($)', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Frecuencia', fontsize=11)\n",
    "axes[1, 1].set_title('Distribución de Amount por Clase', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].set_xlim([0, 500])\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estadísticas de Amount por clase\n",
    "print(\"Estadísticas de Amount por clase:\")\n",
    "print(\"\\nTransacciones Normales:\")\n",
    "print(df[df['Class'] == 0]['Amount'].describe())\n",
    "print(\"\\nTransacciones Fraudulentas:\")\n",
    "print(df[df['Class'] == 1]['Amount'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correlación de variables principales\n",
    "# Seleccionamos algunas variables V para visualizar\n",
    "features_to_plot = ['Time', 'Amount', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'Class']\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df[features_to_plot].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Matriz de Correlación (Variables Seleccionadas)', fontsize=15, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlación de todas las variables con Class\n",
    "correlations_with_class = df.corr()['Class'].sort_values(ascending=False)\n",
    "print(\"Correlación de variables con Class (10 más altas):\")\n",
    "print(correlations_with_class.head(11))  # 11 porque incluye Class consigo misma\n",
    "print(\"\\nCorrelación de variables con Class (10 más bajas):\")\n",
    "print(correlations_with_class.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de las correlaciones más fuertes\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_correlations = correlations_with_class[1:16]  # Top 15 (excluyendo Class)\n",
    "colors_corr = ['red' if x < 0 else 'green' for x in top_correlations.values]\n",
    "top_correlations.plot(kind='barh', color=colors_corr)\n",
    "plt.xlabel('Correlación con Class', fontsize=12)\n",
    "plt.title('Top 15 Variables con Mayor Correlación (Absoluta) con Class', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones de la Comprensión de Datos\n",
    "\n",
    "1. **Dataset limpio:** No hay valores nulos ni duplicados\n",
    "2. **Desbalance severo:** Solo 0.172% de transacciones son fraudulentas (492 de 284,807)\n",
    "3. **Variables transformadas:** V1-V28 son componentes principales (PCA), por lo que no tienen interpretación directa\n",
    "4. **Características originales:** Time y Amount son las únicas variables no transformadas\n",
    "5. **Patrones detectados:** \n",
    "   - Los fraudes tienden a tener montos menores en promedio\n",
    "   - Algunas variables V muestran correlación significativa con fraude (V14, V4, V11, V17)\n",
    "6. **Desafío principal:** El desbalance extremo requerirá técnicas especiales de balanceo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Preparación de los Datos\n",
    "\n",
    "En esta fase limpiaremos y transformaremos los datos para que sean adecuados para el modelado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear copia del dataset para trabajar\n",
    "df_processed = df.copy()\n",
    "\n",
    "print(\"Dataset copiado para procesamiento\")\n",
    "print(f\"Shape: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalización de Time y Amount\n",
    "# Usamos RobustScaler porque es menos sensible a outliers\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "df_processed['Time_scaled'] = scaler.fit_transform(df_processed['Time'].values.reshape(-1, 1))\n",
    "df_processed['Amount_scaled'] = scaler.fit_transform(df_processed['Amount'].values.reshape(-1, 1))\n",
    "\n",
    "# Eliminar las columnas originales\n",
    "df_processed.drop(['Time', 'Amount'], axis=1, inplace=True)\n",
    "\n",
    "print(\"✓ Variables Time y Amount escaladas\")\n",
    "print(f\"Shape actual: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar el resultado del escalado\n",
    "print(\"Estadísticas después del escalado:\")\n",
    "print(df_processed[['Time_scaled', 'Amount_scaled']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar características (X) y variable objetivo (y)\n",
    "X = df_processed.drop('Class', axis=1)\n",
    "y = df_processed['Class']\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"\\nDistribución de y:\")\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# División del dataset en entrenamiento y prueba (80-20)\n",
    "# Usamos stratify para mantener la proporción de clases\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"División del dataset:\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "print(\"\\nDistribución en conjunto de entrenamiento:\")\n",
    "print(y_train.value_counts())\n",
    "print(y_train.value_counts(normalize=True) * 100)\n",
    "\n",
    "print(\"\\nDistribución en conjunto de prueba:\")\n",
    "print(y_test.value_counts())\n",
    "print(y_test.value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Técnicas de Balanceo de Clases\n",
    "\n",
    "Debido al desbalance extremo, implementaremos tres estrategias:\n",
    "1. **Sin balanceo** (baseline)\n",
    "2. **SMOTE** (Synthetic Minority Over-sampling Technique)\n",
    "3. **Random Under-sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estrategia 1: Dataset sin balanceo (baseline)\n",
    "X_train_original = X_train.copy()\n",
    "y_train_original = y_train.copy()\n",
    "\n",
    "print(\"Dataset original (sin balanceo):\")\n",
    "print(f\"X_train shape: {X_train_original.shape}\")\n",
    "print(f\"Distribución: {y_train_original.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estrategia 2: SMOTE (Oversampling de la clase minoritaria)\n",
    "smote = SMOTE(random_state=42, sampling_strategy=0.5)  # Balanceo parcial\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Dataset con SMOTE:\")\n",
    "print(f\"X_train_smote shape: {X_train_smote.shape}\")\n",
    "print(f\"Distribución: {pd.Series(y_train_smote).value_counts().to_dict()}\")\n",
    "print(f\"Proporción: {pd.Series(y_train_smote).value_counts(normalize=True).to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estrategia 3: Random Under-sampling (Reducir la clase mayoritaria)\n",
    "rus = RandomUnderSampler(random_state=42, sampling_strategy=0.5)\n",
    "X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Dataset con Random Under-sampling:\")\n",
    "print(f\"X_train_rus shape: {X_train_rus.shape}\")\n",
    "print(f\"Distribución: {pd.Series(y_train_rus).value_counts().to_dict()}\")\n",
    "print(f\"Proporción: {pd.Series(y_train_rus).value_counts(normalize=True).to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de las tres estrategias\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "strategies = [\n",
    "    (y_train_original, 'Sin Balanceo'),\n",
    "    (y_train_smote, 'Con SMOTE'),\n",
    "    (y_train_rus, 'Con Under-sampling')\n",
    "]\n",
    "\n",
    "for idx, (y_data, title) in enumerate(strategies):\n",
    "    counts = pd.Series(y_data).value_counts().sort_index()\n",
    "    axes[idx].bar(['Normal', 'Fraude'], counts.values, color=['#2ecc71', '#e74c3c'])\n",
    "    axes[idx].set_ylabel('Cantidad', fontsize=11)\n",
    "    axes[idx].set_title(title, fontsize=13, fontweight='bold')\n",
    "    for i, v in enumerate(counts.values):\n",
    "        axes[idx].text(i, v, f'{v:,}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones de la Preparación de Datos\n",
    "\n",
    "1. **Escalado:** Variables Time y Amount normalizadas usando RobustScaler\n",
    "2. **División:** 80% entrenamiento, 20% prueba con estratificación\n",
    "3. **Estrategias de balanceo:**\n",
    "   - Original: 227,451 normales / 394 fraudes\n",
    "   - SMOTE: 227,451 normales / 113,725 fraudes (sintéticos)\n",
    "   - Under-sampling: 788 normales / 394 fraudes\n",
    "4. **Próximo paso:** Entrenar modelos con cada estrategia y comparar resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Modelado\n",
    "\n",
    "Entrenaremos múltiples algoritmos de clasificación con diferentes estrategias de balanceo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Definición de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los modelos a entrenar\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=10),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100, max_depth=15),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100, max_depth=5)\n",
    "}\n",
    "\n",
    "print(\"Modelos definidos:\")\n",
    "for name in models.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Entrenamiento con Dataset Original (Sin Balanceo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario para almacenar modelos entrenados\n",
    "trained_models_original = {}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ENTRENAMIENTO CON DATASET ORIGINAL (SIN BALANCEO)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n Entrenando {name}...\")\n",
    "    model.fit(X_train_original, y_train_original)\n",
    "    trained_models_original[name] = model\n",
    "    print(f\"✓ {name} entrenado\")\n",
    "\n",
    "print(\"\\n✓ Todos los modelos entrenados con dataset original\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Entrenamiento con SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario para almacenar modelos entrenados con SMOTE\n",
    "trained_models_smote = {}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ENTRENAMIENTO CON SMOTE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, model_class in models.items():\n",
    "    print(f\"\\n Entrenando {name}...\")\n",
    "    # Crear nueva instancia del modelo\n",
    "    if name == 'Logistic Regression':\n",
    "        model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    elif name == 'Decision Tree':\n",
    "        model = DecisionTreeClassifier(random_state=42, max_depth=10)\n",
    "    elif name == 'Random Forest':\n",
    "        model = RandomForestClassifier(random_state=42, n_estimators=100, max_depth=15)\n",
    "    else:  # Gradient Boosting\n",
    "        model = GradientBoostingClassifier(random_state=42, n_estimators=100, max_depth=5)\n",
    "    \n",
    "    model.fit(X_train_smote, y_train_smote)\n",
    "    trained_models_smote[name] = model\n",
    "    print(f\"✓ {name} entrenado\")\n",
    "\n",
    "print(\"\\n✓ Todos los modelos entrenados con SMOTE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Entrenamiento con Under-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario para almacenar modelos entrenados con Under-sampling\n",
    "trained_models_rus = {}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ENTRENAMIENTO CON RANDOM UNDER-SAMPLING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, model_class in models.items():\n",
    "    print(f\"\\n Entrenando {name}...\")\n",
    "    # Crear nueva instancia del modelo\n",
    "    if name == 'Logistic Regression':\n",
    "        model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    elif name == 'Decision Tree':\n",
    "        model = DecisionTreeClassifier(random_state=42, max_depth=10)\n",
    "    elif name == 'Random Forest':\n",
    "        model = RandomForestClassifier(random_state=42, n_estimators=100, max_depth=15)\n",
    "    else:  # Gradient Boosting\n",
    "        model = GradientBoostingClassifier(random_state=42, n_estimators=100, max_depth=5)\n",
    "    \n",
    "    model.fit(X_train_rus, y_train_rus)\n",
    "    trained_models_rus[name] = model\n",
    "    print(f\"✓ {name} entrenado\")\n",
    "\n",
    "print(\"\\n✓ Todos los modelos entrenados con Under-sampling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumen del Modelado\n",
    "\n",
    "Se han entrenado 12 modelos en total:\n",
    "- 4 algoritmos diferentes\n",
    "- 3 estrategias de balanceo\n",
    "- Listos para evaluación en la siguiente fase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Evaluación\n",
    "\n",
    "Evaluaremos el rendimiento de todos los modelos usando métricas apropiadas para problemas de clasificación desbalanceada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para evaluar modelos\n",
    "def evaluate_model(model, X_test, y_test, model_name, strategy_name):\n",
    "    \"\"\"\n",
    "    Evalúa un modelo y retorna métricas de rendimiento\n",
    "    \"\"\"\n",
    "    # Predicciones\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calcular métricas\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Matriz de confusión\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    results = {\n",
    "        'Model': model_name,\n",
    "        'Strategy': strategy_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✓ Función de evaluación definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Evaluación de Modelos con Dataset Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar modelos entrenados con dataset original\n",
    "results_original = []\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EVALUACIÓN: MODELOS CON DATASET ORIGINAL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, model in trained_models_original.items():\n",
    "    print(f\"\\nEvaluando {name}...\")\n",
    "    result = evaluate_model(model, X_test, y_test, name, 'Original')\n",
    "    results_original.append(result)\n",
    "    \n",
    "    print(f\"  Accuracy: {result['Accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {result['Precision']:.4f}\")\n",
    "    print(f\"  Recall: {result['Recall']:.4f}\")\n",
    "    print(f\"  F1-Score: {result['F1-Score']:.4f}\")\n",
    "    print(f\"  ROC-AUC: {result['ROC-AUC']:.4f}\")\n",
    "\n",
    "print(\"\\n✓ Evaluación completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Evaluación de Modelos con SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar modelos entrenados con SMOTE\n",
    "results_smote = []\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EVALUACIÓN: MODELOS CON SMOTE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, model in trained_models_smote.items():\n",
    "    print(f\"\\nEvaluando {name}...\")\n",
    "    result = evaluate_model(model, X_test, y_test, name, 'SMOTE')\n",
    "    results_smote.append(result)\n",
    "    \n",
    "    print(f\"  Accuracy: {result['Accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {result['Precision']:.4f}\")\n",
    "    print(f\"  Recall: {result['Recall']:.4f}\")\n",
    "    print(f\"  F1-Score: {result['F1-Score']:.4f}\")\n",
    "    print(f\"  ROC-AUC: {result['ROC-AUC']:.4f}\")\n",
    "\n",
    "print(\"\\n✓ Evaluación completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Evaluación de Modelos con Under-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar modelos entrenados con Under-sampling\n",
    "results_rus = []\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EVALUACIÓN: MODELOS CON UNDER-SAMPLING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, model in trained_models_rus.items():\n",
    "    print(f\"\\nEvaluando {name}...\")\n",
    "    result = evaluate_model(model, X_test, y_test, name, 'Under-sampling')\n",
    "    results_rus.append(result)\n",
    "    \n",
    "    print(f\"  Accuracy: {result['Accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {result['Precision']:.4f}\")\n",
    "    print(f\"  Recall: {result['Recall']:.4f}\")\n",
    "    print(f\"  F1-Score: {result['F1-Score']:.4f}\")\n",
    "    print(f\"  ROC-AUC: {result['ROC-AUC']:.4f}\")\n",
    "\n",
    "print(\"\\n✓ Evaluación completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Comparación de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidar todos los resultados\n",
    "all_results = results_original + results_smote + results_rus\n",
    "\n",
    "# Crear DataFrame con resultados\n",
    "df_results = pd.DataFrame([{\n",
    "    'Model': r['Model'],\n",
    "    'Strategy': r['Strategy'],\n",
    "    'Accuracy': r['Accuracy'],\n",
    "    'Precision': r['Precision'],\n",
    "    'Recall': r['Recall'],\n",
    "    'F1-Score': r['F1-Score'],\n",
    "    'ROC-AUC': r['ROC-AUC']\n",
    "} for r in all_results])\n",
    "\n",
    "# Ordenar por F1-Score\n",
    "df_results_sorted = df_results.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TABLA COMPARATIVA DE RESULTADOS\")\n",
    "print(\"=\"*80)\n",
    "print(df_results_sorted.to_string(index=False))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización comparativa de métricas\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    row = idx // 2\n",
    "    col = idx % 2\n",
    "    \n",
    "    pivot_data = df_results.pivot(index='Model', columns='Strategy', values=metric)\n",
    "    pivot_data.plot(kind='bar', ax=axes[row, col], width=0.8)\n",
    "    \n",
    "    axes[row, col].set_title(f'{metric} por Modelo y Estrategia', \n",
    "                            fontsize=13, fontweight='bold')\n",
    "    axes[row, col].set_ylabel(metric, fontsize=11)\n",
    "    axes[row, col].set_xlabel('Modelo', fontsize=11)\n",
    "    axes[row, col].legend(title='Estrategia', loc='best')\n",
    "    axes[row, col].tick_params(axis='x', rotation=45)\n",
    "    axes[row, col].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de ROC-AUC comparativo\n",
    "plt.figure(figsize=(12, 6))\n",
    "pivot_roc = df_results.pivot(index='Model', columns='Strategy', values='ROC-AUC')\n",
    "pivot_roc.plot(kind='bar', width=0.8, color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "plt.title('Comparación de ROC-AUC por Modelo y Estrategia', fontsize=15, fontweight='bold')\n",
    "plt.ylabel('ROC-AUC Score', fontsize=12)\n",
    "plt.xlabel('Modelo', fontsize=12)\n",
    "plt.legend(title='Estrategia', loc='best')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Matrices de Confusión del Mejor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar el mejor modelo (mayor F1-Score)\n",
    "best_model_idx = df_results_sorted.index[0]\n",
    "best_result = all_results[best_model_idx]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MEJOR MODELO\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Modelo: {best_result['Model']}\")\n",
    "print(f\"Estrategia: {best_result['Strategy']}\")\n",
    "print(f\"F1-Score: {best_result['F1-Score']:.4f}\")\n",
    "print(f\"ROC-AUC: {best_result['ROC-AUC']:.4f}\")\n",
    "print(f\"Recall: {best_result['Recall']:.4f}\")\n",
    "print(f\"Precision: {best_result['Precision']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar matrices de confusión para cada estrategia\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "strategies_results = [\n",
    "    (results_original, 'Original'),\n",
    "    (results_smote, 'SMOTE'),\n",
    "    (results_rus, 'Under-sampling')\n",
    "]\n",
    "\n",
    "# Buscar el mejor modelo dentro de cada estrategia\n",
    "for idx, (results_list, strategy_name) in enumerate(strategies_results):\n",
    "    # Encontrar el mejor modelo de esta estrategia\n",
    "    best_in_strategy = max(results_list, key=lambda x: x['F1-Score'])\n",
    "    \n",
    "    cm = np.array([[best_in_strategy['TN'], best_in_strategy['FP']],\n",
    "                   [best_in_strategy['FN'], best_in_strategy['TP']]])\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                xticklabels=['Normal', 'Fraude'],\n",
    "                yticklabels=['Normal', 'Fraude'])\n",
    "    \n",
    "    axes[idx].set_title(f'{strategy_name}\\n{best_in_strategy[\"Model\"]}\\n(F1: {best_in_strategy[\"F1-Score\"]:.4f})',\n",
    "                       fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Clase Real', fontsize=11)\n",
    "    axes[idx].set_xlabel('Clase Predicha', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Curvas ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar curvas ROC para los mejores modelos de cada estrategia\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for results_list, strategy_name in strategies_results:\n",
    "    best_in_strategy = max(results_list, key=lambda x: x['F1-Score'])\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, best_in_strategy['probabilities'])\n",
    "    roc_auc = best_in_strategy['ROC-AUC']\n",
    "    \n",
    "    plt.plot(fpr, tpr, linewidth=2,\n",
    "             label=f'{strategy_name} - {best_in_strategy[\"Model\"]} (AUC = {roc_auc:.4f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier (AUC = 0.5000)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('Curvas ROC - Mejores Modelos por Estrategia', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Curvas Precision-Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar curvas Precision-Recall para los mejores modelos\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for results_list, strategy_name in strategies_results:\n",
    "    best_in_strategy = max(results_list, key=lambda x: x['F1-Score'])\n",
    "    \n",
    "    precision_curve, recall_curve, _ = precision_recall_curve(\n",
    "        y_test, best_in_strategy['probabilities']\n",
    "    )\n",
    "    \n",
    "    plt.plot(recall_curve, precision_curve, linewidth=2,\n",
    "             label=f'{strategy_name} - {best_in_strategy[\"Model\"]} (F1 = {best_in_strategy[\"F1-Score\"]:.4f})')\n",
    "\n",
    "plt.xlabel('Recall', fontsize=12)\n",
    "plt.ylabel('Precision', fontsize=12)\n",
    "plt.title('Curvas Precision-Recall - Mejores Modelos por Estrategia', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='best', fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8 Análisis Detallado del Mejor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reporte de clasificación del mejor modelo\n",
    "print(\"=\"*60)\n",
    "print(\"REPORTE DETALLADO DEL MEJOR MODELO\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModelo: {best_result['Model']}\")\n",
    "print(f\"Estrategia: {best_result['Strategy']}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MATRIZ DE CONFUSIÓN\")\n",
    "print(\"=\"*60)\n",
    "print(f\"True Negatives (TN):  {best_result['TN']:,}\")\n",
    "print(f\"False Positives (FP): {best_result['FP']:,}\")\n",
    "print(f\"False Negatives (FN): {best_result['FN']:,}\")\n",
    "print(f\"True Positives (TP):  {best_result['TP']:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MÉTRICAS DE RENDIMIENTO\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy:  {best_result['Accuracy']:.4f}\")\n",
    "print(f\"Precision: {best_result['Precision']:.4f}\")\n",
    "print(f\"Recall:    {best_result['Recall']:.4f}\")\n",
    "print(f\"F1-Score:  {best_result['F1-Score']:.4f}\")\n",
    "print(f\"ROC-AUC:   {best_result['ROC-AUC']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"REPORTE DE CLASIFICACIÓN\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, best_result['predictions'], \n",
    "                          target_names=['Normal', 'Fraude']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.9 Interpretación de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de errores del mejor modelo\n",
    "print(\"=\"*60)\n",
    "print(\"ANÁLISIS DE ERRORES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total_fraudes = best_result['TP'] + best_result['FN']\n",
    "fraudes_detectados = best_result['TP']\n",
    "fraudes_no_detectados = best_result['FN']\n",
    "\n",
    "total_normales = best_result['TN'] + best_result['FP']\n",
    "falsos_positivos = best_result['FP']\n",
    "\n",
    "print(f\"\\nDe {total_fraudes} transacciones fraudulentas:\")\n",
    "print(f\"  ✓ Detectadas correctamente: {fraudes_detectados} ({(fraudes_detectados/total_fraudes)*100:.2f}%)\")\n",
    "print(f\"  ✗ No detectadas (FN): {fraudes_no_detectados} ({(fraudes_no_detectados/total_fraudes)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nDe {total_normales} transacciones normales:\")\n",
    "print(f\"  ✗ Marcadas como fraude (FP): {falsos_positivos} ({(falsos_positivos/total_normales)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IMPACTO DE NEGOCIO\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nPorcentaje de fraudes detectados (Recall): {best_result['Recall']*100:.2f}%\")\n",
    "print(f\"Tasa de falsos positivos: {(falsos_positivos/total_normales)*100:.2f}%\")\n",
    "print(f\"\\nInterpretación:\")\n",
    "print(f\"- El modelo detecta {best_result['Recall']*100:.1f}% de los fraudes reales\")\n",
    "print(f\"- Solo {(falsos_positivos/total_normales)*100:.2f}% de transacciones legítimas son bloqueadas incorrectamente\")\n",
    "print(f\"- De cada 100 alertas de fraude, {best_result['Precision']*100:.1f} son fraudes reales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusiones y Recomendaciones\n",
    "\n",
    "### Hallazgos Clave\n",
    "\n",
    "1. **Desafío del Desbalance:**\n",
    "   - El dataset presenta un desbalance extremo (99.83% normales vs 0.17% fraudes)\n",
    "   - Las técnicas de balanceo mejoraron significativamente la detección de fraudes\n",
    "\n",
    "2. **Comparación de Estrategias:**\n",
    "   - **Dataset Original:** Alta precisión pero bajo recall (muchos fraudes no detectados)\n",
    "   - **SMOTE:** Mejor balance entre precision y recall\n",
    "   - **Under-sampling:** Alto recall pero más falsos positivos\n",
    "\n",
    "3. **Métricas Importantes:**\n",
    "   - **Recall (Sensibilidad):** Crítico para detectar fraudes reales\n",
    "   - **Precision:** Importante para no afectar experiencia del cliente con falsos positivos\n",
    "   - **F1-Score:** Mejor métrica para evaluar el balance global\n",
    "\n",
    "### Recomendaciones\n",
    "\n",
    "1. **Para Producción:**\n",
    "   - Implementar el modelo con mejor F1-Score\n",
    "   - Establecer umbrales de probabilidad ajustables según el apetito de riesgo\n",
    "   - Monitorear continuamente el rendimiento del modelo\n",
    "\n",
    "2. **Mejoras Futuras:**\n",
    "   - Probar con otros algoritmos (XGBoost, LightGBM, Neural Networks)\n",
    "   - Implementar feature engineering adicional\n",
    "   - Realizar grid search para optimización de hiperparámetros\n",
    "   - Considerar ensambles de modelos\n",
    "\n",
    "3. **Consideraciones de Negocio:**\n",
    "   - Definir el costo de falsos positivos vs falsos negativos\n",
    "   - Implementar sistema de revisión manual para casos de alta incertidumbre\n",
    "   - Actualizar el modelo periódicamente con nuevos datos\n",
    "\n",
    "### Siguientes Pasos\n",
    "\n",
    "1. **Despliegue:**\n",
    "   - Validar el modelo con datos más recientes\n",
    "   - Implementar en un entorno de staging\n",
    "   - Realizar pruebas A/B antes del despliegue completo\n",
    "\n",
    "2. **Monitoreo:**\n",
    "   - Establecer dashboards de métricas en tiempo real\n",
    "   - Configurar alertas para degradación del modelo\n",
    "   - Documentar casos de error para mejora continua\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "- Dataset: [Credit Card Fraud Detection - Kaggle](https://www.kaggle.com/mlg-ulb/creditcardfraud)\n",
    "- Metodología: CRISP-DM (Cross-Industry Standard Process for Data Mining)\n",
    "- Librería de balanceo: imbalanced-learn\n",
    "- Algoritmos: scikit-learn\n",
    "\n",
    "---\n",
    "\n",
    "**Autor:** [Tu Nombre]  \n",
    "**Fecha:** Noviembre 2024  \n",
    "**Versión:** 1.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
